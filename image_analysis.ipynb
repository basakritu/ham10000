{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c220a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries & Configuration\n",
    "import os, shutil, yaml, six\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Medical Imaging & Feature Extraction\n",
    "# import SimpleITK as sitk\n",
    "# import radiomics\n",
    "# from radiomics import featureextractor\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Machine Learning Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b061a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for file paths\n",
    "\n",
    "image_dir = '../dataset/images/'\n",
    "image_prefix = image_dir + 'ISIC_'\n",
    "label_prefix = '../dataset/HAM10000_segmentations_lesion_tschandl/ISIC_'\n",
    "output_dir = '../dataset/mask_ring/'\n",
    "\n",
    "def visualize_lesion_boundaries(image_list, image_dir, segmentation_dir, kernel_size=50):\n",
    "    \"\"\"\n",
    "    Processes a list of images to visualize the original, dilated, and eroded \n",
    "    segmentation boundaries.\n",
    "    \"\"\"\n",
    "    for img_path in image_list:\n",
    "        # Extract Image ID (e.g., 0024315)\n",
    "        image_id = img_path.split('_')[1].split('.')[0]\n",
    "        \n",
    "        # Construct full paths\n",
    "        full_image_path = os.path.join(image_dir, f\"ISIC_{image_id}.jpg\")\n",
    "        label_path = os.path.join(segmentation_dir, f\"ISIC_{image_id}_segmentation.png\")\n",
    "        \n",
    "        # 1. Load Original Image and Mask\n",
    "        original_img = cv2.imread(full_image_path)\n",
    "        mask = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if original_img is None or mask is None:\n",
    "            print(f\"Error loading files for ID: {image_id}\")\n",
    "            continue\n",
    "\n",
    "        # 2. Perform Morphological Operations (Create the Ring)\n",
    "        # Dilate to capture surrounding skin; Erode to find core\n",
    "        kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "        img_dilation = cv2.dilate(mask, kernel, iterations=1)\n",
    "        img_erosion = cv2.erode(mask, kernel, iterations=1)\n",
    "        \n",
    "        # Calculate the perilesional 'ring' (optional, but useful for radiomics)\n",
    "        perilesional_ring = img_dilation - img_erosion\n",
    "\n",
    "        # 3. Find Contours for Visualization\n",
    "        # Original border\n",
    "        orig_contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # Dilated border\n",
    "        dilated_contours, _ = cv2.findContours(img_dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # Eroded border\n",
    "        eroded_contours, _ = cv2.findContours(img_erosion, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # 4. Draw Overlays\n",
    "        viz_img = original_img.copy()\n",
    "        # Draw Original Boundary in Red\n",
    "        cv2.drawContours(viz_img, orig_contours, -1, (0, 0, 255), 2)\n",
    "        # Draw Dilated and Eroded Boundaries in Cyan/Yellow\n",
    "        cv2.drawContours(viz_img, dilated_contours, -1, (255, 255, 0), 2)\n",
    "        cv2.drawContours(viz_img, eroded_contours, -1, (255, 255, 0), 2)\n",
    "\n",
    "        # 5. Display Result\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(cv2.cvtColor(viz_img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Boundary Visualization: {image_id}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28acce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lesion_rings(image_list):\n",
    "    \"\"\"\n",
    "    Creates 'Ring' masks by dilating original segmentation masks to capture \n",
    "    lesion borders. This helps in identifying high-impact boundary features.\n",
    "    \"\"\"\n",
    "    for img_path in image_list:\n",
    "        # Extract Image ID (e.g., ISIC_0024310)\n",
    "        image_id = img_path.split('_')[1].split('.')[0]\n",
    "        \n",
    "        # Construct Paths\n",
    "        original_img_path = f\"{image_prefix}{image_id}.jpg\"\n",
    "        label_path = f\"{label_prefix}{image_id}_segmentation.png\"\n",
    "        \n",
    "        # Load & Process Mask\n",
    "        mask = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Find original contours (Blue/Red Line)\n",
    "        contours_orig, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Perform Dilation to create the 'Ring'\n",
    "        # Using a 25x25 kernel to capture the surrounding skin context\n",
    "        kernel = np.ones((25, 25), np.uint8)\n",
    "        dilated_mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "        contours_dilated, _ = cv2.findContours(dilated_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Overlay on Original Image\n",
    "        original_img = cv2.imread(img_path)\n",
    "        img_with_viz = original_img.copy()\n",
    "        \n",
    "        # Draw Original Boundary (Red) and Dilated Boundary (Green)\n",
    "        cv2.drawContours(img_with_viz, contours_orig, -1, (0, 0, 255), 2)  # Red\n",
    "        cv2.drawContours(img_with_viz, contours_dilated, -1, (0, 255, 0), 2) # Green\n",
    "        \n",
    "        # Save the processed 'Policy-Ready' visualization\n",
    "        save_path = f\"{output_dir}{image_id}_mask_ring.jpg\"\n",
    "        plt.imsave(save_path, cv2.cvtColor(img_with_viz, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37410050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hollow_core_masks(image_list, segmentation_dir, kernel_size=50):\n",
    "    \"\"\"\n",
    "    Identifies masks where the lesion core is too thin to survive erosion.\n",
    "    Used for data quality control and identifying 'hollow' segmentations.\n",
    "    \"\"\"\n",
    "    hollow_core_ids = []\n",
    "    hollow_count = 0\n",
    "    \n",
    "    # Define a square kernel for morphological operations\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8) \n",
    "\n",
    "    for img_path in image_list:\n",
    "        # Extract ID (e.g., ISIC_0024310)\n",
    "        img_id = img_path.split('_')[1].split('.')[0]\n",
    "        label_path = f\"{segmentation_dir}/ISIC_{img_id}_segmentation.png\"\n",
    "        \n",
    "        # Load mask in grayscale\n",
    "        mask = cv2.imread(label_path, 0)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Erosion removes the outer boundaries of the white (255) pixels\n",
    "            img_erode = cv2.erode(mask, kernel, iterations=1)\n",
    "            \n",
    "            # If no white pixels remain, the lesion has no 'solid core' \n",
    "            # relative to the 50x50 kernel size\n",
    "            if np.sum(img_erode == 255) == 0:\n",
    "                hollow_count += 1\n",
    "                hollow_core_ids.append(img_id)\n",
    "\n",
    "    # Export findings for reproducibility\n",
    "    np.savetxt('hollow_core_indices.txt', hollow_core_ids, delimiter=',', fmt='%s')\n",
    "    \n",
    "    print(f\"Total hollow-core masks found: {hollow_count}\")\n",
    "    return hollow_core_ids\n",
    "\n",
    "\n",
    "# Load the IDs we identified as having \"hollow cores\"\n",
    "\n",
    "hollow_core_ids = np.loadtxt('hollow_core_indices.txt', delimiter=',', dtype=str)\n",
    "\n",
    "# Filter the image list (im_n) to remove these problematic IDs\n",
    "# We only want to extract features from high-quality, solid segmentations\n",
    "filtered_im_n = [path for path in im_n if path.split('_')[1].split('.')[0] not in hollow_core_ids]\n",
    "\n",
    "print(f\"Original images: {len(im_n)}\")\n",
    "print(f\"Images after hollow-core removal: {len(filtered_im_n)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d953379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration & Path Setup\n",
    "PARAMS_PATH = os.path.join(os.getcwd(), 'Params.yaml')\n",
    "OUTPUT_PATH = '../dataset/PyRadiomics_files/radiomics_features_all.csv'\n",
    "\n",
    "with open(PARAMS_PATH, 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "# Initialize Extractor (Done once outside the loop for efficiency)\n",
    "extractor = featureextractor.RadiomicsFeatureExtractor(params)\n",
    "extractor.disableAllFeatures()\n",
    "extractor.enableAllFeatures()\n",
    "\n",
    "features_list = []\n",
    "\n",
    "print(f\"Starting extraction for {len(filtered_im_n)} images...\")\n",
    "\n",
    "# Optimized Feature Extraction Loop\n",
    "for i, img_path in enumerate(filtered_im_n):\n",
    "    try:\n",
    "        # Construct file paths\n",
    "        base_name = img_path.split('.')[0]\n",
    "        mask_path = f\"{base_name}_mask.jpg\"\n",
    "        image_path = f\"{base_name}_or.jpg\"\n",
    "        \n",
    "        # Load images\n",
    "        mask_sitk = sitk.ReadImage(mask_path)\n",
    "        image_sitk = sitk.ReadImage(image_path)\n",
    "        \n",
    "        # Format: Cast to single channel (Grayscale)\n",
    "        # Skin images are RGB; Radiomics requires a 2D scalar image\n",
    "        image_grayscale = sitk.VectorIndexSelectionCast(image_sitk, 0)\n",
    "        \n",
    "        # Execute extraction\n",
    "        # result is a Python OrderedDict\n",
    "        result = extractor.execute(image_grayscale, mask_sitk)\n",
    "        \n",
    "        # Add metadata to the result for identification later\n",
    "        result['image_id'] = img_path\n",
    "        features_list.append(result)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {i}/{len(filtered_im_n)} images...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "# Final Data Processing & Saving\n",
    "# Convert list of OrderedDicts to a pandas DataFrame\n",
    "df_features = pd.DataFrame(features_list)\n",
    "\n",
    "# Filter columns: Keep 'image_id' and all 'original_' radiomics features\n",
    "# This removes diagnostic/header information that Radiomics automatically generates\n",
    "cols_to_keep = ['image_id'] + [c for c in df_features.columns if c.startswith('original_')]\n",
    "df_final = df_features[cols_to_keep]\n",
    "\n",
    "# Save to CSV\n",
    "df_final.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Extraction complete. Results saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Extracted Features ---\n",
    "# (Assuming these CSVs are generated by your extraction loop)\n",
    "combo_ring = pd.read_csv('../dataset/PyRadiomics_files/radiomics_combo_ring_all.csv')\n",
    "lesion = pd.read_csv('../dataset/PyRadiomics_files/radiomics_lesion_all.csv')\n",
    "\n",
    "# --- Merge Features ---\n",
    "# specific 'inner' merge ensures we only keep images that have BOTH feature sets\n",
    "radiomics_features = pd.merge(lesion, combo_ring, on='image_id')\n",
    "\n",
    "# --- Remove Hollow Core Outliers ---\n",
    "# Load the hollow IDs we found earlier\n",
    "try:\n",
    "    hollow_core_ids = np.loadtxt('hollow_core_indices.txt', delimiter=',', dtype=str)\n",
    "    # Filter them out of the feature set\n",
    "    # Note: Adjust logic if image_id format differs in CSV\n",
    "    radiomics_features = radiomics_features[~radiomics_features['image_id'].str.contains('|'.join(hollow_core_ids))]\n",
    "except OSError:\n",
    "    print(\"Warning: hollow_core_indices.txt not found. Skipping hollow core removal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive check for missing values\n",
    "null_summary = radiomics_features.isnull().sum()\n",
    "\n",
    "# Filter to display only features with missing data\n",
    "missing_features = null_summary[null_summary > 0]\n",
    "\n",
    "if not missing_features.empty:\n",
    "    print(\"Missing values detected in the following features:\")\n",
    "    print(missing_features)\n",
    "    \n",
    "else:\n",
    "    print(\"Data Integrity Check Passed: No missing values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470020c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find features with near-zero variance\n",
    "low_variance = radiomics_features.std()[radiomics_features.std() < 0.01]\n",
    "print(f\"Features to consider dropping due to low variance:\\n{low_variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed017703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify low variance features (Standard Deviation < 0.01)\n",
    "# Note: Ensure you only select numeric columns to avoid errors with string IDs\n",
    "numeric_features = radiomics_features.select_dtypes(include=[np.number])\n",
    "low_variance = numeric_features.std()[numeric_features.std() < 0.01]\n",
    "\n",
    "print(f\"Dropping {len(low_variance)} features with low variance.\")\n",
    "\n",
    "# 2. Drop these features from the main dataframe\n",
    "# We use 'errors=ignore' just in case a feature was already dropped\n",
    "radiomics_features.drop(columns=low_variance.index, inplace=True, errors='ignore')\n",
    "\n",
    "# 3. Verify the new shape of the dataset\n",
    "print(f\"Remaining features count: {radiomics_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total count of zeros across the entire dataset\n",
    "total_zeros = (radiomics_features == 0).sum().sum()\n",
    "print(f\"Total zero values in the dataset: {total_zeros}\")\n",
    "\n",
    "# Detailed breakdown: Count of zeros per feature (only showing those with zeros)\n",
    "zeros_per_column = (radiomics_features == 0).sum()\n",
    "features_with_zeros = zeros_per_column[zeros_per_column > 0]\n",
    "\n",
    "if not features_with_zeros.empty:\n",
    "    print(\"\\n--- Zero Values Per Feature ---\")\n",
    "    print(features_with_zeros)\n",
    "else:\n",
    "    print(\"\\nData Integrity Check Passed: No zero values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap for the first 20 features\n",
    "plt.figure(figsize=(12,10))\n",
    "corr_matrix = radiomics_features.iloc[:, :20].corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
    "plt.title('Feature Correlation Heatmap (Checking for Redundancy)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538e7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for redundant features\n",
    "corr_matrix = radiomics_features.iloc[:, :25].corr() # Check a subset for visibility\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap: Identifying Redundant Radiomics Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature/Label Separation\n",
    "y = radiomics_features['dx']\n",
    "X = radiomics_features.drop(['image_id', 'dx'], axis=1)\n",
    "\n",
    "# Scaling: Critical for stable feature importance rankings\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152bc08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Testing Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Initialize and train the classifier\n",
    "# Using a depth of 50 to capture complex non-linear relationships\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=50, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for evaluation\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print high-level metrics\n",
    "print(f\"Classification Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7499a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variance for every extracted radiomics feature\n",
    "variances = X.var()\n",
    "\n",
    "# Sort features from highest variance to lowest\n",
    "# High variance usually indicates a feature that captures significant differences \n",
    "# between different types of lesions.\n",
    "sorted_variances = variances.sort_values(ascending=False)\n",
    "\n",
    "# Visualization of the top 125 features\n",
    "plt.figure(figsize=(12, 6)) # Increased width for better label readability\n",
    "plt.bar(sorted_variances.index[:125], sorted_variances[:125], color='skyblue')\n",
    "\n",
    "# Labeling and Formatting\n",
    "plt.title('Top 125 Features by Variance (Sorted)', fontsize=14)\n",
    "plt.xlabel('Radiomics Features', fontsize=12)\n",
    "plt.ylabel('Variance Value', fontsize=12)\n",
    "\n",
    "# Rotate labels 90 degrees to prevent overlapping\n",
    "plt.xticks(rotation=90, fontsize=8) \n",
    "\n",
    "# Focus the view on a specific range to see the distribution more clearly\n",
    "plt.ylim([0, 3000])\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('Feature_Variance_Analysis.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06288731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "smt = SMOTE(random_state=42)\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Select top 125 features based on variance\n",
    "selected_features = sorted_variances.index[:125]\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "acc_score_kfold = []\n",
    "\n",
    "for fold, (train_ind, test_ind) in enumerate(kf.split(X_selected, y)):\n",
    "    print(f\"--- Processing Fold {fold + 1} ---\")\n",
    "    \n",
    "    # Split Data\n",
    "    X_train, X_test = X_selected.iloc[train_ind, :], X_selected.iloc[test_ind, :]\n",
    "    y_train, y_test = y[train_ind], y[test_ind]\n",
    "    \n",
    "    # Handle Class Imbalance\n",
    "    # SMOTE creates synthetic samples for minority classes in the training set\n",
    "    x_resampled, y_resampled = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Feature Scaling\n",
    "    # Scaler is fit ONLY on training data to prevent data leakage\n",
    "    sc = StandardScaler()\n",
    "    X_train_scaled = sc.fit_transform(x_resampled)\n",
    "    X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "    # Model Training\n",
    "    clf.fit(X_train_scaled, y_resampled)\n",
    "\n",
    "    # Predictions\n",
    "    pred_values = clf.predict(X_test_scaled)\n",
    "    y_pred_proba = clf.predict_proba(X_test_scaled)\n",
    "\n",
    "    # Performance Evaluation\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_test, pred_values)\n",
    "    acc_score_kfold.append(acc)\n",
    "    \n",
    "    # ROC-AUC (One-vs-Rest)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "    \n",
    "    print(f\"Fold {fold+1} Accuracy: {acc:.4f}\")\n",
    "    print(f\"Fold {fold+1} Multi-class ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(classification_report(y_test, pred_values))\n",
    "\n",
    "    # Sensitivity & Specificity calculation per class\n",
    "    res = []\n",
    "    for l in range(7): # 7 classes in HAM10000\n",
    "        # Calculate precision, recall (sensitivity), and support\n",
    "        precision, recall, _, _ = precision_recall_fscore_support(\n",
    "            np.array(y_test) == l, \n",
    "            np.array(pred_values) == l, \n",
    "            pos_label=True, \n",
    "            average=None\n",
    "        )\n",
    "        # recall[0] is Specificity (True Negative Rate), recall[1] is Sensitivity (True Positive Rate)\n",
    "        res.append([l, recall[1], recall[0]])\n",
    "    \n",
    "    print(\"Class-wise Sensitivity and Specificity:\")\n",
    "    print(pd.DataFrame(res, columns=['class', 'sensitivity', 'specificity']))\n",
    "\n",
    "    # Visualization\n",
    "    xp = confusion_matrix(y_test, pred_values)\n",
    "    plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
    "    plot_confusion_matrix(xp, plot_labels) # Assumes plot_confusion_matrix is defined\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a170eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237ad36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
